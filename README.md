# Лабораторная работа №8  
## Параллелизация явной схемы для одномерного уравнения теплопроводности

**Студент:** [Ваше ФИО]  
**Группа:** ИУ7-65Б  
**Дата:** 2025-10-28  
**Дисциплина:** Параллельные вычисления  

---

## 1. Цель работы
Освоить методы распараллеливания алгоритмов решения уравнений в частных производных на примере **явной схемы** для **одномерного нелинейного уравнения теплопроводности**.  
Изучить:
- Распределение данных
- Организацию коммуникаций
- Сравнение эффективности подходов

---

## 2. Теоретическая часть

### 2.1. Математическая постановка
Рассматривается начально-краевая задача:

$$
\begin{cases}
\varepsilon \frac{\partial^2 u}{\partial x^2} - \frac{\partial u}{\partial t} = -u \frac{\partial u}{\partial x} - u^3, & x \in (0,1), \, t \in (0,T], \\
u(0,t) = -1, \quad u(1,t) = 1, \\
u(x,0) = \sin(3\pi(x - 1/6)).
\end{cases}
$$

**Явная разностная схема**:

$$
u_n^{m+1} = u_n^m + \varepsilon \frac{\tau}{h^2} (u_{n+1}^m - 2u_n^m + u_{n-1}^m) + \frac{\tau}{2h} u_n^m (u_{n+1}^m - u_{n-1}^m) + \tau (u_n^m)^3
$$

**Условие устойчивости** (для линейной части):

$$
\tau \leq \frac{h^2}{2\varepsilon}, \quad \varepsilon = 10^{-1.5} \approx 0.03162
$$

Применено:  
$\tau = 0.8 \cdot \frac{h^2}{2\varepsilon}$

---

### 2.2. Используемые MPI-функции

| Функция | Назначение |
|--------|-----------|
| `MPI.COMM_WORLD.Get_size()/Get_rank()` | Определение числа процессов и ранга |
| `MPI.COMM_WORLD.bcast()` | Рассылка параметров (N, M, τ, h) |
| `MPI.COMM_WORLD.Scatterv()` / `Gatherv()` | Распределение и сбор данных |
| `MPI.COMM_WORLD.Sendrecv()` | Обмен ореолами |
| `MPI.COMM_WORLD.Create_cart()` | 1D-картезиевая топология |
| `MPI.Cartcomm.Shift()` | Определение соседей |

---

## 3. Реализация

### 3.1. Структура
- `serial.py` — последовательная версия
- `scatter_gather.py` — `Scatterv` + `Gatherv`
- `cart_sendrecv.py` — `Sendrecv` + `Create_cart`

### 3.2. Особенности
- **Два слоя по времени** → экономия памяти
- **Ореолы** → обмен только границами
- **Автоматический выбор `τ`** → устойчивость
- **Прогресс-бар** → контроль выполнения

### 3.3. Запуск
```bash
# Установка
pip install mpi4py numpy matplotlib

# Последовательная
python3 serial.py

# Параллельные
mpirun --oversubscribe -np 4 python3 cart_sendrecv.py
mpirun --oversubscribe -np 8 python3 scatter_gather.py

4. Эксперименты
4.1. Параметры

N = 400 → h = 0.0025
ε = 10^{-1.5}
τ = 7.91 \times 10^{-5} → M = 75895 → T = 6.0
Для Scatter/Gather: T = 0.6 → M = 7590

4.2. Условия

WSL2 (Ubuntu), Python 3.10, OpenMPI 4.1.5
По 1 запуску (стабильные результаты)


4.3. Результаты
Таблица 1. Время выполнения (сек)









































ПроцессовПоследовательнаяCart+SendrecvScatter/Gather (T=0.6)165.39——2—43.404.364—40.474.048—26.672.8016—58.483.54
Таблица 2. Ускорение (относительно 65.39 сек)



































ПроцессовCart+SendrecvScatter/Gather (T=0.6)11.00×—21.51×15.0×41.62×16.2×82.45×23.4×161.12×18.5×

Примечание: Scatter/Gather при T=0.6 — не сравним по нагрузке.


5. Визуализация
5.1. Время выполнения
<img src="images/execution_time.png" alt="Время выполнения">
5.2. Ускорение
<img src="images/speedup.png" alt="Ускорение">
5.3. Эффективность
<img src="images/efficiency.png" alt="Эффективность">

6. Анализ
6.1. Производительность

Cart+Sendrecv даёт устойчивое ускорение до 8 процессов
Максимум: 2.45× на 8 процессах
Падение на 16: мало работы на процесс

6.2. Узкие места





















ФакторВлияниеКоммуникации2 значения × 75_895 разPython GILБлокирует циклыWSL2Высокая латентность MPI
6.3. Сравнение подходов




















ПодходКоммуникацииМасштабируемостьScatterv/GathervВесь слойПлохаяSendrecv + Cart2 значенияХорошая

7. Контрольные вопросы

Почему Scatterv/Gatherv менее эффективны?
→ Передают весь слой, а не ореолы → O(N) данных.
Зачем нужна виртуальная топология?
→ Упрощает код, автоматически определяет соседей.
Почему ускорение падает на 16 процессах?
→ N_local ≈ 25 → коммуникации доминируют.
Как обеспечить устойчивость?
→ $\tau \leq 0.8 \cdot \frac{h^2}{2\varepsilon}$


8. Заключение
8.1. Выводы

Реализованы три версии
Обеспечена устойчивость
Получено ускорение до 2.45×
Лучший подход: Sendrecv + Create_cart

8.2. Проблемы и решения

















ПроблемаРешениеoverflowАвтоматический τЗависание MPIMPI.Finalize(), --oversubscribe
8.3. Перспективы

Numba → ускорение циклов
C + MPI → реальная производительность
Неявная схема → большой τ


9. Приложения
9.1. Файлы

serial.py, scatter_gather.py, cart_sendrecv.py
plot_results.py — генерация графиков
results.txt — логи

9.2. Библиотеки

Python 3.10+
mpi4py 3.1.5
NumPy 1.24+
OpenMPI 4.1.5

9.3. Литература

Параллельные вычисления — МГТУ, 2023
MPI: The Complete Reference — 1998
Numerical Methods for PDEs — LeVeque
